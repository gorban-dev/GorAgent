/**
 * –¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è Document Indexer
 * –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: node test-document-indexer.js
 */

const DocumentIndexer = require('./document-indexer');

// –ü—Ä–∏–º–µ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
const sampleDocuments = [
    {
        content: `# –í–≤–µ–¥–µ–Ω–∏–µ –≤ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (Machine Learning, ML) ‚Äî —ç—Ç–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –∏–∑—É—á–∞–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏, –ø–æ–∑–≤–æ–ª—è—é—â–∏–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏ –±–µ–∑ —è–≤–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π.

## –¢–∏–ø—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è

### 1. –û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º (Supervised Learning)
–ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –≥–¥–µ –∫–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä –∏–º–µ–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –æ–∂–∏–¥–∞–µ–º—ã–π –≤—ã—Ö–æ–¥.

–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è:
- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
- –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏
- –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–Ω

### 2. –û–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è (Unsupervised Learning)
–ú–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è:
- –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤
- –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
- –°–∂–∞—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö

### 3. –û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Reinforcement Learning)
–ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å–æ —Å—Ä–µ–¥–æ–π, –ø–æ–ª—É—á–∞—è –Ω–∞–≥—Ä–∞–¥—ã –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è.

–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è:
- –ò–≥—Ä–æ–≤—ã–µ AI
- –†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞
- –ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã–µ —Å—Ä–µ–¥—Å—Ç–≤–∞`,
        metadata: {
            name: 'ML-Introduction.md',
            type: 'markdown',
            category: 'education'
        }
    },
    {
        content: `# –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ NLP

–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ ‚Äî —ç—Ç–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –û–Ω–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç –º–æ–¥–µ–ª—è–º –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ç–µ–∫—Å—Ç–æ–º, –ø—Ä–µ–æ–±—Ä–∞–∑—É—è –µ–≥–æ –≤ —á–∏—Å–ª–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã.

## –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≤–∏—Ç–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

### Word2Vec (2013)
–ü–µ—Ä–≤–∞—è –ø–æ–ø—É–ª—è—Ä–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è word embeddings. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–∞ –ø–æ–¥—Ö–æ–¥–∞:
- CBOW (Continuous Bag of Words) - –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–æ–≤–æ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
- Skip-gram - –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ —Å–ª–æ–≤—É

### GloVe (2014)
Global Vectors for Word Representation. –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ —Å–ª–æ–≤.

### FastText (2016)
–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ Word2Vec, –∫–æ—Ç–æ—Ä–æ–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—é —Å–ª–æ–≤. –†–∞–∑–±–∏–≤–∞–µ—Ç —Å–ª–æ–≤–∞ –Ω–∞ n-–≥—Ä–∞–º–º—ã.

### BERT (2018)
Bidirectional Encoder Representations from Transformers. –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ —É—á–∏—Ç—ã–≤–∞—é—Ç –æ–∫—Ä—É–∂–µ–Ω–∏–µ —Å–ª–æ–≤–∞.

### OpenAI Embeddings (2022)
–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á:
- text-embedding-ada-002
- text-embedding-3-small
- text-embedding-3-large

## –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

1. **–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫** - –ø–æ–∏—Å–∫ –ø–æ —Å–º—ã—Å–ª—É, –∞ –Ω–µ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
2. **–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤** - –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
3. **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã** - —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ö–æ–¥—Å—Ç–≤–∞
4. **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞** - –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–µ–∫—Å—Ç–∞
5. **RAG (Retrieval-Augmented Generation)** - —É–ª—É—á—à–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ LLM –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º

## –ú–µ—Ç—Ä–∏–∫–∏ —Å—Ö–æ–¥—Å—Ç–≤–∞

–î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É—é—Ç:
- **–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ** - —É–≥–æ–ª –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏
- **–ï–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ** - —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ
- **–°–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ** - –ø—Ä—è–º–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤`,
        metadata: {
            name: 'Embeddings-Guide.md',
            type: 'markdown',
            category: 'nlp'
        }
    },
    {
        content: `# Neural Networks Basics

Neural networks are computing systems inspired by biological neural networks. They consist of interconnected nodes (neurons) organized in layers.

## Architecture

### Input Layer
Receives the initial data for processing.

### Hidden Layers
Intermediate layers that transform the input into something that the output layer can use.

### Output Layer
Produces the final prediction or classification.

## Activation Functions

Common activation functions:
- **ReLU** (Rectified Linear Unit): f(x) = max(0, x)
- **Sigmoid**: f(x) = 1 / (1 + e^(-x))
- **Tanh**: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))
- **Softmax**: Used for multi-class classification

## Training Process

1. **Forward Propagation**: Input data flows through the network
2. **Loss Calculation**: Compare prediction with actual value
3. **Backpropagation**: Calculate gradients of the loss
4. **Weight Update**: Adjust weights using optimization algorithm

## Common Architectures

- **CNN** (Convolutional Neural Networks) - for images
- **RNN** (Recurrent Neural Networks) - for sequences
- **LSTM** (Long Short-Term Memory) - improved RNN
- **Transformer** - attention-based architecture`,
        metadata: {
            name: 'Neural-Networks-Basics.md',
            type: 'markdown',
            category: 'deep-learning'
        }
    }
];

async function runTests() {
    console.log('\n' + '='.repeat(60));
    console.log('DOCUMENT INDEXER ‚Äî –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï');
    console.log('='.repeat(60) + '\n');

    // –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä indexer
    const indexer = new DocumentIndexer({
        chunkSize: 300,  // –ú–µ–Ω—å—à–∏–π —Ä–∞–∑–º–µ—Ä –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        chunkOverlap: 30,
        indexPath: './test-document-index.json'
    });

    console.log('üìã –ù–∞—Å—Ç—Ä–æ–π–∫–∏:');
    console.log(`  –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: ${indexer.chunkSize} —Ç–æ–∫–µ–Ω–æ–≤`);
    console.log(`  –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ: ${indexer.chunkOverlap} —Ç–æ–∫–µ–Ω–æ–≤`);
    console.log(`  –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: ${indexer.model}\n`);

    try {
        // –¢–µ—Å—Ç 1: –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        console.log('\n' + '‚îÄ'.repeat(60));
        console.log('–¢–ï–°–¢ 1: –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤');
        console.log('‚îÄ'.repeat(60));

        for (const doc of sampleDocuments) {
            console.log(`\nüìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞: ${doc.metadata.name}`);
            const result = await indexer.processDocument(doc.content, doc.metadata);
            console.log(`‚úÖ –°–æ–∑–¥–∞–Ω–æ ${result.chunks.length} —á–∞–Ω–∫–æ–≤, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ ${result.totalTokens} —Ç–æ–∫–µ–Ω–æ–≤`);
        }

        // –¢–µ—Å—Ç 2: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        console.log('\n' + '‚îÄ'.repeat(60));
        console.log('–¢–ï–°–¢ 2: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–∞');
        console.log('‚îÄ'.repeat(60));

        const stats = indexer.getStats();
        console.log('\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:');
        console.log(`  –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: ${stats.totalDocuments}`);
        console.log(`  –ß–∞–Ω–∫–æ–≤: ${stats.totalChunks}`);
        console.log(`  –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: ${stats.avgChunkSize} —Ç–æ–∫–µ–Ω–æ–≤`);
        console.log(`  –†–∞–∑–º–µ—Ä –∏–Ω–¥–µ–∫—Å–∞: ${(stats.indexSize / 1024).toFixed(2)} KB`);

        // –¢–µ—Å—Ç 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        console.log('\n' + '‚îÄ'.repeat(60));
        console.log('–¢–ï–°–¢ 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞');
        console.log('‚îÄ'.repeat(60));

        const saveResult = await indexer.saveIndex();
        console.log(`\nüíæ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω: ${saveResult.path}`);
        console.log(`  –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: ${(saveResult.size / 1024).toFixed(2)} KB`);

        // –¢–µ—Å—Ç 4: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        console.log('\n' + '‚îÄ'.repeat(60));
        console.log('–¢–ï–°–¢ 4: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫');
        console.log('‚îÄ'.repeat(60));

        const queries = [
            '—á—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ',
            '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏',
            '–≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤',
            'supervised learning examples'
        ];

        for (const query of queries) {
            console.log(`\nüîç –ó–∞–ø—Ä–æ—Å: "${query}"`);
            const searchResult = await indexer.search(query, 3);
            
            console.log(`üìä –ù–∞–π–¥–µ–Ω–æ ${searchResult.results.length} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:\n`);
            
            searchResult.results.forEach((result, i) => {
                console.log(`  ${i + 1}. –î–æ–∫—É–º–µ–Ω—Ç: ${result.chunk.metadata.documentName}`);
                console.log(`     –°—Ö–æ–¥—Å—Ç–≤–æ: ${(result.similarity * 100).toFixed(1)}%`);
                console.log(`     –¢–µ–∫—Å—Ç: ${result.chunk.text.substring(0, 100)}...`);
                console.log('');
            });
        }

        // –¢–µ—Å—Ç 5: –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞
        console.log('\n' + '‚îÄ'.repeat(60));
        console.log('–¢–ï–°–¢ 5: –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞');
        console.log('‚îÄ'.repeat(60));

        // –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏
        const indexer2 = new DocumentIndexer({
            indexPath: './test-document-index.json'
        });

        const loadResult = await indexer2.loadIndex();
        console.log(`\nüìÇ –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω: ${loadResult.path}`);
        console.log(`  –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: ${loadResult.documents}`);
        console.log(`  –ß–∞–Ω–∫–æ–≤: ${loadResult.chunks}`);

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–æ–∏—Å–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
        console.log('\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏...');
        const testSearch = await indexer2.search('neural networks', 2);
        console.log(`‚úÖ –ù–∞–π–¥–µ–Ω–æ ${testSearch.results.length} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤`);

        // –¢–µ—Å—Ç 6: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        console.log('\n' + '‚îÄ'.repeat(60));
        console.log('–¢–ï–°–¢ 6: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤');
        console.log('‚îÄ'.repeat(60));

        const documents = indexer.getDocuments();
        console.log(`\nüìö –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: ${documents.length}\n`);
        
        documents.forEach((doc, i) => {
            console.log(`  ${i + 1}. ${doc.name}`);
            console.log(`     –ß–∞–Ω–∫–æ–≤: ${doc.chunksCount}`);
            console.log(`     –¢–æ–∫–µ–Ω–æ–≤: ${doc.totalTokens}`);
            console.log(`     –ö–∞—Ç–µ–≥–æ—Ä–∏—è: ${doc.metadata.category || 'N/A'}`);
            console.log('');
        });

        console.log('\n' + '='.repeat(60));
        console.log('‚úÖ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´ –£–°–ü–ï–®–ù–û');
        console.log('='.repeat(60) + '\n');

    } catch (error) {
        console.error('\n‚ùå –û–®–ò–ë–ö–ê:', error.message);
        console.error('\n–î–µ—Ç–∞–ª–∏:', error);
        process.exit(1);
    }
}

// –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
if (require.main === module) {
    runTests().catch(error => {
        console.error('–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞:', error);
        process.exit(1);
    });
}

module.exports = { runTests };



